"""
RAG ì²´ì¸ ëª¨ë“ˆ
ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ìƒì„±
"""

from typing import List, Dict, Optional
import httpx

# LangChain imports - ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document


class RAGChain:
    """RAG ì²´ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, vector_store_manager, api_key: str, api_type: str = "groq"):
        """
        Args:
            vector_store_manager: VectorStoreManager ì¸ìŠ¤í„´ìŠ¤
            api_key: AI API í‚¤ (GROQ, Gemini ë“±)
            api_type: API íƒ€ì… ('groq', 'gemini', 'gemma')
        """
        self.vector_store = vector_store_manager
        self.api_key = api_key
        self.api_type = api_type.lower()
    
    def _format_context(self, documents: List[Document]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        
        Args:
            documents: ê²€ìƒ‰ëœ Document ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not documents:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            content = doc.page_content.strip()
            
            context_parts.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source}\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str, system_message: Optional[str] = None) -> str:
        """
        RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            system_message: ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„ íƒ)
            
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # ë¬¸ì œ ì¶œì œ ìš”ì²­ ê°ì§€ (ë‹¤ì–‘í•œ íŒ¨í„´)
        query_lower = query.lower()
        is_quiz_request = any(keyword in query_lower for keyword in [
            'ë¬¸ì œ ë‚´', 'ë¬¸ì œë‚´', 'ë¬¸ì œ ì¶œì œ', 'ë¬¸ì œì¶œì œ', 
            'í€´ì¦ˆ', 'quiz', 'ì‹œí—˜', 'í…ŒìŠ¤íŠ¸',
            'ë¬¸ì œ ë§Œë“¤', 'ë¬¸ì œë§Œë“¤', 'ê°ê´€ì‹', 'ì„ íƒí˜•'
        ])
        
        if is_quiz_request:
            # ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ (ê¸°ë³¸ê°’ 5ê°œ)
            import re
            num_match = re.search(r'(\d+)\s*ê°œ', query)
            num_questions = int(num_match.group(1)) if num_match else 5
            num_questions = min(max(num_questions, 1), 20)  # 1~20ê°œ ì œí•œ
            
            system_message = f"""ë‹¹ì‹ ì€ ë°”ì´ì˜¤í—¬ìŠ¤ ë¶„ì•¼ ì „ë¬¸ êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ 4ì§€ì„ ë‹¤í˜• ê°ê´€ì‹ ë¬¸ì œë¥¼ ì¶œì œí•´ì£¼ì„¸ìš”."""
            
            prompt = f"""{system_message}

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ìš”ì²­ ì‚¬í•­ ===
{query}

=== ë¬¸ì œ ì¶œì œ ê·œì¹™ ===
1. **ë¬¸ì œ ê°œìˆ˜**: ì´ {num_questions}ê°œì˜ ë¬¸ì œë¥¼ ì¶œì œí•˜ì„¸ìš”
2. **ë¬¸ì œ í˜•ì‹**: ëª¨ë“  ë¬¸ì œëŠ” 4ì§€ì„ ë‹¤í˜• ê°ê´€ì‹ì…ë‹ˆë‹¤
3. **ë¬¸ì œ êµ¬ì„±**:
   - ë¬¸ì œ ë²ˆí˜¸ì™€ ì§ˆë¬¸ (ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ)
   - â‘  â‘¡ â‘¢ â‘£ 4ê°œì˜ ì„ íƒì§€ (ê° ì„ íƒì§€ëŠ” í•œ ì¤„ì”©)
   - ë¹ˆ ì¤„ (ë¬¸ì œ êµ¬ë¶„)
   - **ì •ë‹µ**: ì •ë‹µ ë²ˆí˜¸ì™€ ë‚´ìš© (ì˜ˆ: **ì •ë‹µ: â‘¡ ì„¤ëª…**)
   - **í•´ì„¤**: ì™œ ì´ê²ƒì´ ì •ë‹µì¸ì§€ ê°„ë‹¨íˆ ì„¤ëª…
   - **ì¶œì²˜**: ë¬¸ì„œëª… ë˜ëŠ” ì°¸ê³  í˜ì´ì§€
   - ë¹ˆ ì¤„ 2ê°œ (ë‹¤ìŒ ë¬¸ì œì™€ êµ¬ë¶„)

4. **ì¶œì œ ì›ì¹™**:
   - ë¬¸ì„œì— ëª…í™•íˆ ë‚˜ì™€ ìˆëŠ” ë‚´ìš©ë§Œ ì¶œì œ
   - ì¶”ì¸¡ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€
   - ì˜¤ë‹µë„ ê·¸ëŸ´ë“¯í•˜ê²Œ ì‘ì„± (ë„ˆë¬´ ëª…ë°±í•œ ì˜¤ë‹µ ê¸ˆì§€)
   - ì¤‘ë³µ ë¬¸ì œ ì¶œì œ ê¸ˆì§€

=== ì¶œì œ ì˜ˆì‹œ ===
1. mRNA ë°±ì‹ ì˜ ì‘ë™ ì›ë¦¬ë¡œ ì˜³ì€ ê²ƒì€?
â‘  ë°”ì´ëŸ¬ìŠ¤ë¥¼ ì§ì ‘ ì£¼ì…í•˜ì—¬ ë©´ì—­ ë°˜ì‘ì„ ìœ ë„í•œë‹¤
â‘¡ ë©”ì‹ ì € RNAë¥¼ ì´ìš©í•˜ì—¬ ì„¸í¬ê°€ íŠ¹ì • ë‹¨ë°±ì§ˆì„ ìƒì„±í•˜ë„ë¡ í•œë‹¤
â‘¢ í•­ì²´ë¥¼ ì§ì ‘ ì£¼ì…í•˜ì—¬ ì¦‰ì‹œ ë©´ì—­ë ¥ì„ ë†’ì¸ë‹¤
â‘£ DNAë¥¼ ë³€í˜•ì‹œì¼œ ë°”ì´ëŸ¬ìŠ¤ì— ì €í•­í•˜ê²Œ ë§Œë“ ë‹¤

**ì •ë‹µ: â‘¡ ë©”ì‹ ì € RNAë¥¼ ì´ìš©í•˜ì—¬ ì„¸í¬ê°€ íŠ¹ì • ë‹¨ë°±ì§ˆì„ ìƒì„±í•˜ë„ë¡ í•œë‹¤**
**í•´ì„¤**: mRNA ë°±ì‹ ì€ ë©”ì‹ ì € RNAë¥¼ ì´ìš©í•˜ì—¬ ìš°ë¦¬ ëª¸ì˜ ì„¸í¬ê°€ íŠ¹ì • ë‹¨ë°±ì§ˆì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
**ì¶œì²˜**: ë°”ì´ì˜¤í—¬ìŠ¤ ê¸°ì´ˆ.pdf


2. ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—…ì˜ ì£¼ìš” ë¶„ì•¼ê°€ ì•„ë‹Œ ê²ƒì€?
â‘  ì‹ ì•½ ê°œë°œ
â‘¡ ì˜ë£Œê¸°ê¸°
â‘¢ ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´
â‘£ ìë™ì°¨ ì œì¡°

**ì •ë‹µ: â‘£ ìë™ì°¨ ì œì¡°**
**í•´ì„¤**: ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—…ì˜ ì£¼ìš” ë¶„ì•¼ëŠ” ì‹ ì•½ ê°œë°œ, ì˜ë£Œê¸°ê¸°, ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ë“±ì´ë©°, ìë™ì°¨ ì œì¡°ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
**ì¶œì²˜**: ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… ê°œìš”.pdf


=== ë¬¸ì œ ì¶œì œ ì‹œì‘ ===
"""
        else:
            # ì¼ë°˜ ì§ˆë¬¸ ì‘ë‹µ
            if system_message is None:
                system_message = """ë‹¹ì‹ ì€ ë°”ì´ì˜¤í—¬ìŠ¤ ë¶„ì•¼ ì „ë¬¸ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
            
            prompt = f"""{system_message}

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì§ˆë¬¸ ===
{query}

=== ë‹µë³€ ê·œì¹™ ===
1. ë¬¸ì„œì— ëª…í™•í•œ ë‹µì´ ìˆìœ¼ë©´ ì •í™•íˆ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ë‹µì´ ì—†ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ì¶”ì¸¡í•˜ê±°ë‚˜ ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
4. êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
5. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ê¸ˆì§€)

=== ë‹µë³€ ===
"""
        
        return prompt
    
    async def _call_groq_api(self, prompt: str) -> str:
        """GROQ API í˜¸ì¶œ"""
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,  # ì •í™•ë„ ìš°ì„ 
            "max_tokens": 1000,
            "top_p": 0.9
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=30.0)
            response.raise_for_status()
            data = response.json()
            
            if data.get('choices'):
                return data['choices'][0]['message']['content']
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def _call_gemini_api(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={self.api_key}"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 1000,
                "topP": 0.9
            }
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=30.0)
            response.raise_for_status()
            data = response.json()
            
            if data.get('candidates'):
                return data['candidates'][0]['content']['parts'][0]['text']
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def query(self, 
                    question: str, 
                    k: int = 5,  # 3ì—ì„œ 5ë¡œ ì¦ê°€
                    system_message: Optional[str] = None,
                    min_similarity: float = 0.3) -> Dict:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì¶”ê°€
        """
        RAG ì§ˆë¬¸ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’ 5ë¡œ ì¦ê°€)
            system_message: ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ ë©”ì‹œì§€
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0~1.0, ê¸°ë³¸ê°’ 0.3)
            
        Returns:
            {
                'answer': AI ì‘ë‹µ,
                'sources': ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸,
                'context': ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            }
        """
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            print(f"[DEBUG] ì§ˆë¬¸: {question}")
            print(f"[DOC] {k}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            
            documents = self.vector_store.search_with_score(question, k=k)
            
            if not documents:
                return {
                    'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    'sources': [],
                    'context': ""
                }
            
            # 2. ìœ ì‚¬ë„ ì²´í¬ - ëª¨ë“  ë¬¸ì„œì˜ ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê²½ê³ 
            max_similarity = max([doc_dict.get('score', 0) for doc_dict in documents])
            print(f"[INFO] ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.2%}")
            
            if max_similarity < min_similarity:
                return {
                    'answer': f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ íŒ: ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•˜ê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.\n(ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.1%})",
                    'sources': [],
                    'context': ""
                }
            
            # 3. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (SimpleVectorStore í˜•ì‹)
            context_parts = []
            for i, doc_dict in enumerate(documents, 1):
                metadata = doc_dict.get('metadata', {})
                source = metadata.get('original_filename') or metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')
                subject = metadata.get('subject', '')
                content = doc_dict.get('content', '').strip()
                similarity = doc_dict.get('score', 0)
                
                source_info = f"{source}"
                if subject:
                    source_info += f" ({subject})"
                
                context_parts.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source_info} (ìœ ì‚¬ë„: {similarity:.1%})\n{content}")
            
            context = "\n\n".join(context_parts)
            
            print(f"[OK] {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            
            # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_prompt(question, context, system_message)
            
            # 5. AI API í˜¸ì¶œ
            print(f"[AI] {self.api_type.upper()} API í˜¸ì¶œ ì¤‘...")
            
            if self.api_type == 'groq' or self.api_type == 'gemma':
                answer = await self._call_groq_api(prompt)
            elif self.api_type == 'gemini':
                answer = await self._call_gemini_api(prompt)
            else:
                answer = "ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…ì…ë‹ˆë‹¤."
            
            print(f"[OK] ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            
            # 6. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (SimpleVectorStore í˜•ì‹)
            sources = []
            for doc_dict in documents:
                metadata = doc_dict.get('metadata', {})
                source_name = metadata.get('original_filename') or metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')
                subject = metadata.get('subject', '')
                
                source_display = source_name
                if subject:
                    source_display = f"{source_name} - {subject}"
                
                sources.append({
                    'source': source_display,
                    'content': doc_dict.get('content', '')[:200] + '...',
                    'similarity': float(doc_dict.get('score', 0)),  # 0~1 ë²”ìœ„ë¡œ ë°˜í™˜
                    'metadata': metadata
                })
            
            return {
                'answer': answer,
                'sources': sources,
                'context': context
            }
            
        except Exception as e:
            print(f"[ERROR] RAG ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'context': ""
            }
    
    async def query_simple(self, question: str, k: int = 3) -> str:
        """
        ê°„ë‹¨í•œ RAG ì§ˆë¬¸ (ë‹µë³€ë§Œ ë°˜í™˜)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            AI ì‘ë‹µ ë¬¸ìì—´
        """
        result = await self.query(question, k=k)
        return result['answer']


if __name__ == "__main__":
    import asyncio
    import os
    from document_loader import DocumentLoader
    from vector_store import VectorStoreManager
    
    async def test_rag():
        # ë¬¸ì„œ ë¡œë”
        loader = DocumentLoader(chunk_size=500, chunk_overlap=50)
        
        # ìƒ˜í”Œ ë¬¸ì„œ
        sample_text = """
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… ê°œìš”
        
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—…ì€ ìƒëª…ê³µí•™ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì¸ê°„ì˜ ê±´ê°•ê³¼ ì‚¶ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ì‚°ì—…ì…ë‹ˆë‹¤.
        ì£¼ìš” ë¶„ì•¼ë¡œëŠ” ì‹ ì•½ ê°œë°œ, ì˜ë£Œê¸°ê¸°, ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.
        
        mRNA ë°±ì‹  ê¸°ìˆ 
        
        mRNA ë°±ì‹ ì€ ë©”ì‹ ì € RNAë¥¼ ì´ìš©í•˜ì—¬ ìš°ë¦¬ ëª¸ì˜ ì„¸í¬ê°€ íŠ¹ì • ë‹¨ë°±ì§ˆì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
        ì´ ê¸°ìˆ ì€ COVID-19 íŒ¬ë°ë¯¹ ë™ì•ˆ ë¹ ë¥´ê²Œ ë°œì „í•˜ì˜€ìœ¼ë©°, í–¥í›„ ì•” ì¹˜ë£Œ ë“±ì—ë„ í™œìš©ë  ì „ë§ì…ë‹ˆë‹¤.
        """
        
        os.makedirs("./test_docs", exist_ok=True)
        with open("./test_docs/sample.txt", "w", encoding="utf-8") as f:
            f.write(sample_text)
        
        # ë¬¸ì„œ ë¡œë“œ
        docs = loader.load_document("./test_docs/sample.txt", {"subject": "ë°”ì´ì˜¤í—¬ìŠ¤ ê¸°ì´ˆ"})
        
        # ë²¡í„° ìŠ¤í† ì–´
        vector_store = VectorStoreManager(
            persist_directory="./test_chroma_db",
            collection_name="test_collection"
        )
        vector_store.add_documents(docs)
        
        # RAG ì²´ì¸ (API í‚¤ í•„ìš”)
        api_key = os.getenv("GROQ_API_KEY", "your-api-key-here")
        rag_chain = RAGChain(vector_store, api_key, api_type="groq")
        
        # ì§ˆë¬¸
        question = "mRNA ë°±ì‹ ì´ ë¬´ì—‡ì¸ê°€ìš”?"
        print(f"\n[Q] ì§ˆë¬¸: {question}\n")
        
        result = await rag_chain.query(question, k=2)
        
        print(f"\n[AI] ë‹µë³€:\n{result['answer']}\n")
        print(f"\n[DOC] ì°¸ê³  ë¬¸ì„œ:")
        for i, source in enumerate(result['sources'], 1):
            print(f"\n  {i}. {source['source']} (ìœ ì‚¬ë„: {source['similarity']:.4f})")
            print(f"     {source['content']}")
    
    # ì‹¤í–‰
    asyncio.run(test_rag())
