"""
RAG ì²´ì¸ ëª¨ë“ˆ
ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ìƒì„±
"""

from typing import List, Dict, Optional
import httpx
from langchain.schema import Document


class RAGChain:
    """RAG ì²´ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, vector_store_manager, api_key: str, api_type: str = "groq"):
        """
        Args:
            vector_store_manager: VectorStoreManager ì¸ìŠ¤í„´ìŠ¤
            api_key: AI API í‚¤ (GROQ, Gemini ë“±)
            api_type: API íƒ€ì… ('groq', 'gemini', 'gemma')
        """
        self.vector_store = vector_store_manager
        self.api_key = api_key
        self.api_type = api_type.lower()
    
    def _format_context(self, documents: List[Document]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        
        Args:
            documents: ê²€ìƒ‰ëœ Document ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not documents:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            content = doc.page_content.strip()
            
            context_parts.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source}\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str, system_message: Optional[str] = None) -> str:
        """
        RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            system_message: ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„ íƒ)
            
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        if system_message is None:
            system_message = """ë‹¹ì‹ ì€ ë°”ì´ì˜¤í—¬ìŠ¤ ë¶„ì•¼ ì „ë¬¸ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."""
        
        prompt = f"""{system_message}

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì§ˆë¬¸ ===
{query}

=== ë‹µë³€ ===
ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:"""
        
        return prompt
    
    async def _call_groq_api(self, prompt: str) -> str:
        """GROQ API í˜¸ì¶œ"""
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,  # ì •í™•ë„ ìš°ì„ 
            "max_tokens": 1000,
            "top_p": 0.9
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=30.0)
            response.raise_for_status()
            data = response.json()
            
            if data.get('choices'):
                return data['choices'][0]['message']['content']
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def _call_gemini_api(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={self.api_key}"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 1000,
                "topP": 0.9
            }
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=30.0)
            response.raise_for_status()
            data = response.json()
            
            if data.get('candidates'):
                return data['candidates'][0]['content']['parts'][0]['text']
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def query(self, 
                    question: str, 
                    k: int = 3,
                    filter: Optional[Dict] = None,
                    system_message: Optional[str] = None) -> Dict:
        """
        RAG ì§ˆë¬¸ ì²˜ë¦¬
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            filter: ë©”íƒ€ë°ì´í„° í•„í„°
            system_message: ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ ë©”ì‹œì§€
            
        Returns:
            {
                'answer': AI ì‘ë‹µ,
                'sources': ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸,
                'context': ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            }
        """
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            print(f"ğŸ” ì§ˆë¬¸: {question}")
            print(f"ğŸ“š {k}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            
            documents = self.vector_store.search_with_score(question, k=k, filter=filter)
            
            if not documents:
                return {
                    'answer': "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    'sources': [],
                    'context': ""
                }
            
            # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            docs_only = [doc for doc, score in documents]
            context = self._format_context(docs_only)
            
            print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            
            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_prompt(question, context, system_message)
            
            # 4. AI API í˜¸ì¶œ
            print(f"ğŸ¤– {self.api_type.upper()} API í˜¸ì¶œ ì¤‘...")
            
            if self.api_type == 'groq' or self.api_type == 'gemma':
                answer = await self._call_groq_api(prompt)
            elif self.api_type == 'gemini':
                answer = await self._call_gemini_api(prompt)
            else:
                answer = "ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…ì…ë‹ˆë‹¤."
            
            print(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            
            # 5. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            sources = []
            for doc, score in documents:
                sources.append({
                    'source': doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                    'content': doc.page_content[:200] + '...',
                    'similarity': float(score),
                    'metadata': doc.metadata
                })
            
            return {
                'answer': answer,
                'sources': sources,
                'context': context
            }
            
        except Exception as e:
            print(f"âŒ RAG ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'context': ""
            }
    
    async def query_simple(self, question: str, k: int = 3) -> str:
        """
        ê°„ë‹¨í•œ RAG ì§ˆë¬¸ (ë‹µë³€ë§Œ ë°˜í™˜)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            AI ì‘ë‹µ ë¬¸ìì—´
        """
        result = await self.query(question, k=k)
        return result['answer']


if __name__ == "__main__":
    import asyncio
    import os
    from document_loader import DocumentLoader
    from vector_store import VectorStoreManager
    
    async def test_rag():
        # ë¬¸ì„œ ë¡œë”
        loader = DocumentLoader(chunk_size=500, chunk_overlap=50)
        
        # ìƒ˜í”Œ ë¬¸ì„œ
        sample_text = """
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… ê°œìš”
        
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—…ì€ ìƒëª…ê³µí•™ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì¸ê°„ì˜ ê±´ê°•ê³¼ ì‚¶ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ì‚°ì—…ì…ë‹ˆë‹¤.
        ì£¼ìš” ë¶„ì•¼ë¡œëŠ” ì‹ ì•½ ê°œë°œ, ì˜ë£Œê¸°ê¸°, ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.
        
        mRNA ë°±ì‹  ê¸°ìˆ 
        
        mRNA ë°±ì‹ ì€ ë©”ì‹ ì € RNAë¥¼ ì´ìš©í•˜ì—¬ ìš°ë¦¬ ëª¸ì˜ ì„¸í¬ê°€ íŠ¹ì • ë‹¨ë°±ì§ˆì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
        ì´ ê¸°ìˆ ì€ COVID-19 íŒ¬ë°ë¯¹ ë™ì•ˆ ë¹ ë¥´ê²Œ ë°œì „í•˜ì˜€ìœ¼ë©°, í–¥í›„ ì•” ì¹˜ë£Œ ë“±ì—ë„ í™œìš©ë  ì „ë§ì…ë‹ˆë‹¤.
        """
        
        os.makedirs("./test_docs", exist_ok=True)
        with open("./test_docs/sample.txt", "w", encoding="utf-8") as f:
            f.write(sample_text)
        
        # ë¬¸ì„œ ë¡œë“œ
        docs = loader.load_document("./test_docs/sample.txt", {"subject": "ë°”ì´ì˜¤í—¬ìŠ¤ ê¸°ì´ˆ"})
        
        # ë²¡í„° ìŠ¤í† ì–´
        vector_store = VectorStoreManager(
            persist_directory="./test_chroma_db",
            collection_name="test_collection"
        )
        vector_store.add_documents(docs)
        
        # RAG ì²´ì¸ (API í‚¤ í•„ìš”)
        api_key = os.getenv("GROQ_API_KEY", "your-api-key-here")
        rag_chain = RAGChain(vector_store, api_key, api_type="groq")
        
        # ì§ˆë¬¸
        question = "mRNA ë°±ì‹ ì´ ë¬´ì—‡ì¸ê°€ìš”?"
        print(f"\nğŸ’¬ ì§ˆë¬¸: {question}\n")
        
        result = await rag_chain.query(question, k=2)
        
        print(f"\nğŸ¤– ë‹µë³€:\n{result['answer']}\n")
        print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
        for i, source in enumerate(result['sources'], 1):
            print(f"\n  {i}. {source['source']} (ìœ ì‚¬ë„: {source['similarity']:.4f})")
            print(f"     {source['content']}")
    
    # ì‹¤í–‰
    asyncio.run(test_rag())
