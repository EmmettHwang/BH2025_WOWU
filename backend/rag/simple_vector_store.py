"""
ê°„ì†Œí™”ëœ RAG ë²¡í„° ìŠ¤í† ì–´ (FAISS ê¸°ë°˜, ChromaDB ì—†ì´)
Python 3.14 í˜¸í™˜
"""
import os
import pickle
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np


class SimpleVectorStore:
    """FAISS ê¸°ë°˜ ê°„ë‹¨í•œ ë²¡í„° ìŠ¤í† ì–´"""
    
    def __init__(
        self,
        collection_name: str = "documents",
        persist_directory: str = "./simple_vector_db",
        embedding_model: str = "jhgan/ko-sroberta-multitask"
    ):
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_model_name = embedding_model
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(persist_directory, exist_ok=True)
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        self.embedding_dimension = self.embedding_model.get_sentence_embedding_dimension()
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.index = faiss.IndexFlatL2(self.embedding_dimension)
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.documents = []
        self.metadatas = []
        
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        self._load_index()
        
        print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(self.documents)})")
    
    def add_documents(
        self,
        texts: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None
    ) -> List[str]:
        """ë¬¸ì„œ ì¶”ê°€"""
        if metadatas is None:
            metadatas = [{}] * len(texts)
        
        # ì„ë² ë”© ìƒì„±
        print(f"ğŸ”„ {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(embeddings.astype('float32'))
        
        # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        document_ids = []
        for i, (text, metadata) in enumerate(zip(texts, metadatas)):
            doc_id = f"doc_{len(self.documents) + i}"
            document_ids.append(doc_id)
            
            self.documents.append(text)
            self.metadatas.append({
                **metadata,
                "document_id": doc_id
            })
        
        # ì¸ë±ìŠ¤ ì €ì¥
        self._save_index()
        
        print(f"âœ… {len(texts)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
        return document_ids
    
    def similarity_search(
        self,
        query: str,
        k: int = 3
    ) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if len(self.documents) == 0:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_model.encode(
            [query],
            convert_to_numpy=True
        ).astype('float32')
        
        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(query_embedding, min(k, len(self.documents)))
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for distance, idx in zip(distances[0], indices[0]):
            if idx < len(self.documents):
                results.append({
                    "content": self.documents[idx],
                    "metadata": self.metadatas[idx],
                    "score": float(1 / (1 + distance))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                })
        
        return results
    
    def get_all_documents(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        return [
            {
                "content": doc,
                "metadata": meta
            }
            for doc, meta in zip(self.documents, self.metadatas)
        ]
    
    def clear(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        self.index = faiss.IndexFlatL2(self.embedding_dimension)
        self.documents = []
        self.metadatas = []
        self._save_index()
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def count(self) -> int:
        """ë¬¸ì„œ ê°œìˆ˜"""
        return len(self.documents)
    
    def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        index_path = os.path.join(self.persist_directory, f"{self.collection_name}.index")
        metadata_path = os.path.join(self.persist_directory, f"{self.collection_name}.pkl")
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.index, index_path)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(metadata_path, 'wb') as f:
            pickle.dump({
                'documents': self.documents,
                'metadatas': self.metadatas,
                'embedding_model': self.embedding_model_name
            }, f)
    
    def _load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_path = os.path.join(self.persist_directory, f"{self.collection_name}.index")
        metadata_path = os.path.join(self.persist_directory, f"{self.collection_name}.pkl")
        
        if os.path.exists(index_path) and os.path.exists(metadata_path):
            try:
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(index_path)
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(metadata_path, 'rb') as f:
                    data = pickle.load(f)
                    self.documents = data['documents']
                    self.metadatas = data['metadatas']
                
                print(f"âœ… ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(self.documents)})")
            except Exception as e:
                print(f"âš ï¸  ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
